{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8f9423-1bf4-401f-951f-5439a4a9d90a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T08:38:07.722770Z",
     "iopub.status.busy": "2025-10-14T08:38:07.722283Z",
     "iopub.status.idle": "2025-10-14T08:38:18.140286Z",
     "shell.execute_reply": "2025-10-14T08:38:18.139697Z",
     "shell.execute_reply.started": "2025-10-14T08:38:07.722747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved: [{'idx': 1, 'score': 1.2429592609405518, 'text': 'FAISS is a fast similarity search library from Facebook AI Research.'}, {'idx': 2, 'score': 1.8776800632476807, 'text': 'Chroma is an open source vector database oriented for LangChain integrations.'}]\n",
      "\n",
      "assistant answer:\n",
      " FAISS is a fast similarity search library developed by Facebook AI Research. It is designed to efficiently search for similar items in large datasets.\n",
      "{'query': 'How does Chroma integrate with LangChain?', 'result': 'Chroma integrates with LangChain to store and retrieve document embeddings efficiently. It provides persistent vector storage and similarity search capabilities, which are essential for AI pipelines.'}\n",
      "\n",
      "Filtered result (topic='hybrid'):\n",
      " [Document(id='82e35bbd-599e-4ac3-8843-b05524e075ae', metadata={'topic': 'hybrid'}, page_content='Chroma allows hybrid lexical + semantic search.'), Document(id='2a7e37e8-89f3-40fd-8bc3-856874c21929', metadata={'topic': 'hybrid'}, page_content='Chroma allows hybrid lexical + semantic search.')]\n",
      "\n",
      "✅ Advanced Chroma RAG demo complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrieval-Augmented Generation (RAG) — Hands-on notebook\n",
    "By Pavan Susarla — practical lessons: Easy -> Medium -> Hard\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# COMMON IMPORTS & CONFIG\n",
    "# -----------------------------\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"myeky\"\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# UTIL: simple chunking\n",
    "# -----------------------------\n",
    "def simple_chunk_text(\n",
    "    text: str, chunk_size: int = 1000, overlap: int = 200\n",
    ") -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap if end < n else n\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LESSON 1 — Minimal RAG (FAISS + OpenAI)\n",
    "# -----------------------------\n",
    "EMB_MODEL = \"all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "docs = [\n",
    "    \"LangChain helps you build applications with LLMs by connecting components.\",\n",
    "    \"FAISS is a fast similarity search library from Facebook AI Research.\",\n",
    "    \"Chroma is an open source vector database oriented for LangChain integrations.\",\n",
    "]\n",
    "\n",
    "# Create embeddings\n",
    "vectors = embedder.encode(docs, convert_to_numpy=True)\n",
    "vec_dim = vectors.shape[1]\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatL2(vec_dim)\n",
    "index.add(vectors)\n",
    "\n",
    "\n",
    "def faiss_search(query: str, top_k: int = 2):\n",
    "    qv = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(qv, top_k)\n",
    "    return [\n",
    "        {\"idx\": int(i), \"score\": float(D[0][j]), \"text\": docs[int(i)]}\n",
    "        for j, i in enumerate(I[0])\n",
    "    ]\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant. Use the following retrieved documents as context (do not invent facts).\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_answer_openai(question: str, retrieved_texts: List[str]):\n",
    "    context = \"\\n---\\n\".join(retrieved_texts)\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "\n",
    "# Test minimal RAG\n",
    "q = \"What is FAISS?\"\n",
    "res = faiss_search(q)\n",
    "print(\"retrieved:\", res)\n",
    "ans = generate_answer_openai(q, [r[\"text\"] for r in res])\n",
    "print(\"\\nassistant answer:\\n\", ans)\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# -----------------------------\n",
    "# LESSON 2 — LangChain + Chroma (real-world RAG)\n",
    "# -----------------------------\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "doc_text = \"\"\"LangChain integrates with Chroma to store and retrieve document embeddings efficiently.\n",
    "Chroma provides persistent vector storage and similarity search for AI pipelines.\"\"\"\n",
    "\n",
    "# 1️⃣ Split document into chunks\n",
    "chunker = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "docs = chunker.create_documents([doc_text])\n",
    "\n",
    "# 2️⃣ Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3️⃣ Initialize Chroma DB\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_demo\",\n",
    "    persist_directory=\"./chroma_store\",\n",
    ")\n",
    "retriever = chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# 4️⃣ Initialize new LLM client (from langchain_openai)\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    \"mykey\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 5️⃣ Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "question = \"How does Chroma integrate with LangChain?\"\n",
    "response = qa_chain.invoke({\"query\": question})\n",
    "print(response)\n",
    "\n",
    "# -----------------------------\n",
    "# LESSON 3 — Advanced Chroma (metadata, hybrid, evaluation)\n",
    "# -----------------------------\n",
    "meta_docs = [\n",
    "    (\"Chroma supports metadata-based retrieval.\", {\"topic\": \"metadata\"}),\n",
    "    (\"Chroma allows hybrid lexical + semantic search.\", {\"topic\": \"hybrid\"}),\n",
    "]\n",
    "meta_texts = [d[0] for d in meta_docs]\n",
    "meta_metadata = [d[1] for d in meta_docs]\n",
    "meta_vectors = embeddings.embed_documents(meta_texts)\n",
    "\n",
    "chroma_db.add_texts(texts=meta_texts, metadatas=meta_metadata)\n",
    "\n",
    "filtered = chroma_db.similarity_search(\n",
    "    \"What is hybrid search?\", k=2, filter={\"topic\": \"hybrid\"}\n",
    ")\n",
    "print(\"\\nFiltered result (topic='hybrid'):\\n\", filtered)\n",
    "\n",
    "\n",
    "def recall_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:\n",
    "    topk = set(retrieved_ids[:k])\n",
    "    return len(topk.intersection(set(relevant_ids))) / len(relevant_ids)\n",
    "\n",
    "\n",
    "print(\"\\n✅ Advanced Chroma RAG demo complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb33cd5-fab7-4bd5-88f1-962b4c2845bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
